# ะัะธะผะตั ะบะพะดะฐ ะดะปั ะฟัะพะณะฝะพะทะฐ ั ะธัะฟะพะปัะทะพะฒะฐะฝะธะตะผ ะพะฑััะตะฝะฝะพะน ะผะพะดะตะปะธ
import numpy as np
from sklearn.datasets import load_iris, make_regression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, mean_squared_error


# ะะฑััะฐััะธะต ะดะฐะฝะฝัะต
X_train = np.array([[1], [2], [3], [4], [5]])
y_train = np.array([1.5, 3.2, 4.8, 6.5, 8.1])

# ะกะพะทะดะฐะตะผ ะธ ะพะฑััะฐะตะผ ะผะพะดะตะปั
model = LinearRegression()
model.fit(X_train, y_train)

# ะัะตะดัะบะฐะทัะฒะฐะตะผ ะดะปั ะฝะพะฒะพะณะพ ะทะฝะฐัะตะฝะธั
X_new = np.array([[6]])
y_pred = model.predict(X_new)
print(f'ะัะตะดัะบะฐะทะฐะฝะฝะพะต ะทะฝะฐัะตะฝะธะต ะดะปั X=6: {y_pred[0]:.2f}')


# ะะฑััะฐััะธะต ะดะฐะฝะฝัะต (ััะธ ะฟัะตะดะธะบัะพัะฐ)
X_train = np.array([
    [1, 2, 3],
    [2, 3, 4],
    [3, 4, 5],
    [4, 5, 6],
    [5, 6, 7]
])
y_train = np.array([1.5, 3.2, 4.8, 6.5, 8.1])  # ะฆะตะปะตะฒะฐั ะฟะตัะตะผะตะฝะฝะฐั

# ะกะพะทะดะฐะตะผ ะธ ะพะฑััะฐะตะผ ะผะพะดะตะปั
model = LinearRegression()
model.fit(X_train, y_train)

# ะัะตะดัะบะฐะทัะฒะฐะตะผ ะดะปั ะฝะพะฒะพะณะพ ะทะฝะฐัะตะฝะธั (ััะธ ะฟัะตะดะธะบัะพัะฐ)
X_new = np.array([[6, 7, 8]])
y_pred = model.predict(X_new)

print(f'ะัะตะดัะบะฐะทะฐะฝะฝะพะต ะทะฝะฐัะตะฝะธะต ะดะปั X_new={X_new.tolist()[0]}: {y_pred[0]:.2f}')


# TODO IRIS
# ะะปะฐััะธัะธะบะฐัะธั
# ะะฐะณััะทะบะฐ ะดะฐะฝะฝัั
data = load_iris()
X, y = data.data, data.target

# ะะฐะทะดะตะปะตะฝะธะต ะดะฐะฝะฝัั
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ะะฑััะตะฝะธะต ะผะพะดะตะปะธ
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# ะัะตะฝะบะฐ ะผะพะดะตะปะธ
train_accuracy = accuracy_score(y_train, clf.predict(X_train))
test_accuracy = accuracy_score(y_test, clf.predict(X_test))

print(f"Train Accuracy: {train_accuracy:.2f}")
print(f"Test Accuracy: {test_accuracy:.2f}")

# ะะตะณัะตััะธั
# ะะตะฝะตัะฐัะธั ะดะฐะฝะฝัั
X, y = make_regression(n_samples=100, n_features=1, noise=0.1)

# ะะฐะทะดะตะปะตะฝะธะต ะดะฐะฝะฝัั
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ะะฑััะตะฝะธะต ะผะพะดะตะปะธ
reg = LinearRegression()
reg.fit(X_train, y_train)

# ะัะตะฝะบะฐ ะผะพะดะตะปะธ
train_mse = mean_squared_error(y_train, reg.predict(X_train))
test_mse = mean_squared_error(y_test, reg.predict(X_test))

print(f"Train MSE: {train_mse:.2f}")
print(f"Test MSE: {test_mse:.2f}")


# TODO ะะพะธัะบ ะจัะผะฐ ัะตัะตะท z-score
#  z-score - ะพัะบะปะพะฝะตะฝะธะต ะทะฝะฐัะตะฝะธะน ะฒัะฑะพัะบะธ ะพั ััะตะดะฝะตะณะพ ะฒ ัะธะณะผะฐั
#  z = xiโฮผ / ฯ
A = np.array([5,   5,   1,   6,   3,   1,   2,   4,   4,   6,   4,   9,   3,
         3,   8,   3,   1,   3,   8,   5, 100])
z = (A - np.mean(A)) / np.std(A)  # ๐ z = xiโฮผ / ฯ
outliers = A[np.abs(z) > 3]  # ๐ ะตัะปะธ ะฑะพะปััะต 3 ัะธะณะผ ะพั ัะตะฝััะฐ

print(outliers)