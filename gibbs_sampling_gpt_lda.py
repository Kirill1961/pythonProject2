from gensim import corpora
from gensim.models import LdaModel
from gensim.utils import simple_preprocess
import random

# TODO 1
# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É –≤–∞—Å –µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
# data = [
#     ["apple", "banana", "banana", "lemon"],
#     ["apple", "banana", "peach"],
#     ["apple", "banana", "banana", "peach"],
#     ["lemon", "orange", "lemon", "lemon"],
# ]

# TODO 2
data_raw = [
    "–∫–æ—Ç –ª—é–±–∏—Ç –º–æ–ª–æ–∫–æ",
    "—Å–æ–±–∞–∫–∞ –ª—é–±–∏—Ç –∫–æ—Å—Ç—å",
    "–∫–æ—Ç –∏ —Å–æ–±–∞–∫–∞ –¥—Ä—É–∑—å—è"
]

# TODO Tokenizer
# 1Ô∏è‚É£üëé
# data = list(map(lambda x: x.split(' '), data_raw))
# 2Ô∏è‚É£üëÜ
# data = [doc.split(" ") for doc in data_raw]
# 3Ô∏è‚É£üöÄ
data = [simple_preprocess(doc) for doc in data_raw]


#  TODO 3
d = [
    ["MongoDB", "data science", "Spark", "Postgres", "pandas", "NoSQL" "Big Data"],
    ["Storm", "Java", "pandas", "MongoDB", "data science", "pandas", "data science"],
    [
        "C++",
        "Scikit-learn",
        "regression",
        "neural network",
        "MongoDB",
        "Big Data",
        "data science",
        "NoSQL Big Data",
    ],
    ["statistic", "R", "go", "scipy", "numpy", "MongoDB", "pandas", "data science"],
    [
        "Storm",
        "regression",
        "neural network",
        "MongoDB",
        "Big Data",
        "data science",
        "NoSQL Big Data",
    ],
    ["C++", "Scikit-learn", "regression", "neural network", "Big Data", "pandas"],
    [
        "Scikit-learn",
        "regression",
        "neural network",
        "MongoDB",
        "Big Data",
        "data science",
        "NoSQL Big Data",
    ],
    [
        "statistic",
        "R",
        "go",
        "scipy",
        "numpy",
        "data science",
        "pandas",
        "NoSQL" "Big Data",
    ],
    [
        "Python",
        "Hadoop",
        "numpy",
        "NoSQL",
        "MongoDB",
        "HBase",
        "data science",
        "NoSQL" "Big Data",
    ],
    [
        "Cassandra",
        "machine learning",
        "Haskel",
        "C++",
        "scipy",
        "data science",
        "NoSQL Big Data",
    ],
    ["Python", "Hadoop", "numpy", "NoSQL Big Data", "pandas", "NoSQL Big Data"],
    ["statistic", "Java", "pandas", "MongoDB", "data science"],
    ["numpy", "decision trees", "libsvm", "MongoDB", "probability"],
    [
        "statistic",
        "R",
        "go",
        "scipy",
        "numpy",
        "machine learning",
        "data science",
        "NoSQL Big Data",
    ],
    ["Python", "Hadoop", "numpy", "data science", "MongoDB", "NoSQL" "Big Data"],
    ["HBase", "Storm", "Java", "pandas"],
    [
        "statistic",
        "R",
        "go",
        "scipy",
        "C++",
        "MongoDB",
        "pandas",
        "data science",
        "NoSQL Big Data",
    ],
    ["Spark", "Postgres", "Cassandra", "machine learning", "Haskel", "pandas"],
    ["statistic", "R", "go", "scipy", "HBase", "pandas", "data science"],
    ["Python", "Hadoop", "numpy", "NoSQL", "HBase", "NoSQL Big Data"],
]

# TODO 4
# data_raw = [
#     [
#         "Human machine interface for lab abc computer applications",
#         "A survey of user opinion of computer system response time",
#         "The EPS user interface management system",
#         "System and human system engineering testing of EPS",
#         "Relation of user perceived response time to error measurement",
#         "The generation of random binary unordered trees",
#         "The intersection graph of paths in trees",
#         "Graph minors IV Widths of trees and well quasi ordering",
#         "Graph minors A survey",
#     ]
# ]

# TODO split - –î–µ–ª–∏–º —Å–ª–æ–≤–∞ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º,
#  –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–¥–µ–ª—ë–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
# data = [doc.split(" ") for doc in data_raw[0]]
# print('Docums+terms :\n', data)

# TODO –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è, –ø—è—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ,
#  Dictionary<5 unique tokens: ['apple', 'banana', 'lemon', 'peach', 'orange']>
dictionary = corpora.Dictionary(data)  # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç unique tokens –≤ id (–∏–Ω–¥–µ–∫—Å—ã)
print("Dictionary :\n", dictionary, "\n")

# TODO doc2bow –≤—Å–µ —Å–ª–æ–≤–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ bag-of-words, (id, frequents)
corpus = [dictionary.doc2bow(text) for text in data]
print("corpus –∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ \n "
      "( idx_term x frequents_term ): \n" , corpus, "\n")

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LDA
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3)
print("lda_model :\n", lda_model, "\n")

print("lda_model.print_topics : \n ", lda_model.print_topics(), "\n")


# TODO üö© –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ Variational Bayes üëâ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–¥–∫–∞–ø–æ—Ç–æ–º LdaModel
#  –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–¥–Ω–æ–º—É –º–∏–∫—Ä–æ—à–∞–≥—É, –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª—å –¥–µ–ª–∞–µ—Ç –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
# doc = corpus[0]
for idx, _ in enumerate(corpus):
    doc = corpus[idx]
    topic_probabilities = lda_model.get_document_topics(doc, minimum_probability=0.01)
    print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å topics –≤ document ‚Ññ {idx} :\n", topic_probabilities, "\n")

topic_distribution = [probability for _, probability in topic_probabilities]
print("topic_distribution : \n", topic_distribution, "\n")

# TODO –í—ã–±–æ—Ä –Ω–æ–≤–æ–π —Ç–µ–º—ã —Å —É—á–µ—Ç–æ–º —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
new_topic = random.choices(range(lda_model.num_topics), weights=topic_distribution)
print("new_topic : \n", new_topic, "\n")
print("New topic for the document: \n", new_topic, "\n")
